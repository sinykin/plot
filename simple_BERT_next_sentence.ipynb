{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using BERT for next sentence prediction\n",
    "\n",
    "The hardest part of this is making sure you've got the python packages you need installed. You'll need to install ```torch``` and ```transformers,``` and as usual with python, you may run into compatibility issues.\n",
    "\n",
    "All I can say to help there is \"google the error message\"?\n",
    "\n",
    "But once you've got the packages installed it's easy.\n",
    "\n",
    "First we load everything and get it ready to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "built tokenizer\n",
      "built model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print('built tokenizer')\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "print('built model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then here's a function to do next sentence prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(firstsentence, secondsentence):\n",
    "    global tokenizer, model\n",
    "\n",
    "    encoding = tokenizer.encode_plus(firstsentence, secondsentence, return_tensors = 'pt', max_seq_length = 255)\n",
    "    loss, logits = model(**encoding, next_sentence_label=torch.LongTensor([1]))\n",
    "\n",
    "    return loss, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need two sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(12.3876, grad_fn=<NllLossBackward>),\n",
       " tensor([[ 6.2713, -6.1164]], grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstsentence = \"I was walking to the store one day to buy groceries.\"\n",
    "secondsentence = \"At the store I bought bananas and milk.\"\n",
    "\n",
    "get_logits(firstsentence, secondsentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. What the hell does that mean? The first line is the \"loss,\" the second the \"logits.\" The relation between logits and probability makes my head hurt to explain, so I'm just going to [point at Wikipedia.](https://en.wikipedia.org/wiki/Logit)\n",
    "\n",
    "But for a quick and dirty approach I wrote this function which *loosely* translates BERT's logits output into a probability for the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_probability(firstsent, secondsent):\n",
    "    '''\n",
    "    \n",
    "    :param logits: a tensor produced by BERT\n",
    "    :return: probability of the first category after softmax\n",
    "    '''\n",
    "    loss, logits = get_logits(firstsent, secondsent)\n",
    "    \n",
    "    poslogit = logits[0, 0]\n",
    "    neglogit = logits[0, 1]\n",
    "\n",
    "    pospart = math.pow(2.72, poslogit)\n",
    "    negpart = math.pow(2.72, neglogit)\n",
    "\n",
    "    posprob = pospart / (pospart + negpart)\n",
    "\n",
    "    return posprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999958626257772"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstsentence = \"I was walking to the store one day to buy groceries.\"\n",
    "secondsentence = \"At the store I bought bananas and milk.\"\n",
    "\n",
    "get_probability(firstsentence, secondsentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, now we can see that BERT considers that a pretty probable sequence. Let's try a less probable sequence.\n",
    "\n",
    "We'll use the same first sentence about walking to the store, and for our second sentence\n",
    "\n",
    "    Psychedelics are a hallucinogenic class of psychoactive drug whose primary effect is to trigger non-ordinary states of consciousness and psychedelic experiences via serotonin 2A receptor agonism.\n",
    "    \n",
    "Which is from Wikipedia on \"psychedelic drug.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.017845137195324e-05"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstsentence = \"I was walking to the store one day to buy groceries.\"\n",
    "secondsentence = \"Psychedelics are a hallucinogenic class of psychoactive drug whose primary effect is to trigger non-ordinary states of consciousness and psychedelic experiences via serotonin 2A receptor agonism.\"\n",
    "get_probability(firstsentence, secondsentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a much less probable sequence! Let's try a slightly weaker non-sequitur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.055498046096809694"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstsentence = \"I was walking to the store one day to buy groceries.\"\n",
    "secondsentence = \"Everything is closed due to the pandemic.\"\n",
    "get_probability(firstsentence, secondsentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that probability is slightly higher. Still unlikely. But not *totally* improbable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
